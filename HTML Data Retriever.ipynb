{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textract\n",
    "#!pip install colorama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import textract\n",
    "import requests\n",
    "import colorama\n",
    "from bs4 import BeautifulSoup\n",
    "from operator import itemgetter\n",
    "from urllib.parse import urljoin,urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataFromFile(path):\n",
    "  text = textract.process(path)\n",
    "  text = text.decode(\"utf-8\") \n",
    "  text=str(text).replace('\\n',' ')\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SinglePagePreprocessedContent(url):\n",
    "    reqs = requests.get(url)\n",
    "    soup = BeautifulSoup(reqs.text, features=\"html.parser\")\n",
    "    dictionary = {}\n",
    "\n",
    "    for tag in soup.find_all(True):\n",
    "        if tag.name in dictionary:\n",
    "            dictionary[tag.name] +=1\n",
    "        else:\n",
    "            dictionary[tag.name] = 1\n",
    "\n",
    "    keys=list(dictionary.keys())\n",
    "    newDict={}\n",
    "    if 'html' in dictionary.keys():\n",
    "          keys.remove('html')\n",
    "    if 'head' in dictionary.keys():\n",
    "          keys.remove('head')\n",
    "\n",
    "    if 'body' in dictionary.keys():\n",
    "          keys.remove('body')\n",
    "\n",
    "    if 'script' in dictionary.keys():\n",
    "          keys.remove('script')\n",
    "\n",
    "    if 'meta' in dictionary.keys():\n",
    "          keys.remove('meta')\n",
    "\n",
    "    if 'link' in dictionary.keys():\n",
    "          keys.remove('link')\n",
    "\n",
    "    if 'style' in dictionary.keys():\n",
    "          keys.remove('style')    \n",
    "    \n",
    "    if 'mp4' in dictionary.keys():\n",
    "          keys.remove('mp4')    \n",
    "    \n",
    "    if 'img' in dictionary.keys():\n",
    "          keys.remove('img')    \n",
    "\n",
    "\n",
    "    for tag in keys:\n",
    "        data=soup.find_all(tag)\n",
    "        newDict[tag]=list()        \n",
    "        for textData in data:\n",
    "            value=textData.text\n",
    "            value=re.sub(\"\\s\\s+\" , \" \", value)\n",
    "            value=value.strip()\n",
    "            value=re.sub(\"\\n\" , \" \", value)\n",
    "            if '((function()' not in value:\n",
    "              newDict[tag].append(value)\n",
    "        newDict[tag]=set(newDict[tag])        \n",
    "        newDict[tag]=list(filter(lambda a: a != '', newDict[tag]))\n",
    "        \n",
    "            \n",
    "    return newDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DefineDictionaryHTML():\n",
    "  Dictionary={\n",
    "      'Page Link': '',\n",
    "      'Title':'',\n",
    "      'Content':{},\n",
    "  }\n",
    "  return Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DictionaryFillerHTML(url):\n",
    "  Dictionary=DefineDictionaryHTML()\n",
    "  Dictionary['Page Link']=url\n",
    "  HTMLDict=SinglePagePreprocessedContent(url)\n",
    "  if 'title' in HTMLDict.keys():\n",
    "    Dictionary['Title']=HTMLDict['title']\n",
    "  Dictionary['Content']=HTMLDict\n",
    "\n",
    "  return Dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(url):\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "\n",
    "def get_all_website_links(url):\n",
    "\n",
    "    urls = set()\n",
    "    domain_name = urlparse(url).netloc\n",
    "    soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
    "    for a_tag in soup.findAll(\"a\"):\n",
    "        href = a_tag.attrs.get(\"href\")\n",
    "        if href == \"\" or href is None:\n",
    "            continue\n",
    "        href = urljoin(url, href)\n",
    "        parsed_href = urlparse(href)\n",
    "        href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
    "        if not is_valid(href):\n",
    "            continue\n",
    "        if href in internal_urls:\n",
    "            continue\n",
    "        if domain_name not in href:\n",
    "            if href not in external_urls:\n",
    "                print(f\"{GRAY}[!] External link: {href}{RESET}\")\n",
    "                external_urls.add(href)\n",
    "            continue\n",
    "        print(f\"{GREEN}[*] Internal link: {href}{RESET}\")\n",
    "        urls.add(href)\n",
    "        internal_urls.add(href)\n",
    "    return urls\n",
    "\n",
    "\n",
    "def crawl(url):\n",
    "    global total_urls_visited\n",
    "    total_urls_visited += 1\n",
    "    print(f\"{YELLOW}[*] Crawling: {url}{RESET}\")\n",
    "    links = get_all_website_links(url)\n",
    "    for link in links:\n",
    "        crawl(link)\n",
    "url=\"https://www.anna-dsb.com/\"\n",
    "colorama.init()\n",
    "GREEN = colorama.Fore.GREEN\n",
    "GRAY = colorama.Fore.LIGHTBLACK_EX\n",
    "RESET = colorama.Fore.RESET\n",
    "YELLOW = colorama.Fore.YELLOW\n",
    "total_urls_visited = 0\n",
    "internal_urls = set()\n",
    "external_urls = set()\n",
    "crawl(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links=internal_urls\n",
    "delLinks=links\n",
    "FinalData=[]\n",
    "for link in links:\n",
    "  if '.mp4' in link:\n",
    "    delLinks.remove(link)\n",
    "\n",
    "links=delLinks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in links:\n",
    "  FinalData.append(DictionaryFillerHTML(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Folder_Name=\"WebScrapper\"\n",
    "os.mkdir(Folder_Name)\n",
    "path=os.path.join(os.getcwd(),Folder_Name)\n",
    "DataInOneFile=\"DataInOneFile\"\n",
    "DataInSeperatedFile=\"DataInSeperatedFile\"\n",
    "pathDataInOneFile=os.path.join(path,DataInOneFile)\n",
    "pathDataInSeperatedFile=os.path.join(path,DataInSeperatedFile)\n",
    "os.mkdir(pathDataInOneFile)\n",
    "os.mkdir(pathDataInSeperatedFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataInOneFile\n",
    "File_Name=\"data.json\"\n",
    "npath=os.path.join(pathDataInOneFile,File_Name)\n",
    "with open(npath, \"w\") as outfile:\n",
    "    json.dump(FinalData, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataInSeperatedFile\n",
    "\n",
    "count=1\n",
    "for link in links:\n",
    "    File_Name=str(count)+\".json\"\n",
    "    mpath=os.path.join(pathDataInSeperatedFile,File_Name)\n",
    "    with open(mpath, \"w\") as outfile:\n",
    "        json.dump(FinalData[count-1], outfile)\n",
    "    count=count+1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
